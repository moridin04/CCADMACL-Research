{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu12S4xsIs7Ti0N7Eg793C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moridin04/CCADMACL-Research/blob/main/fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Fraud_Detection_Program_FINAL.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1J94F9wWFUPZsZqWEwFvx67fh0TM9zAsQ\n",
        "\n",
        "# **Exploring Anomaly Detection Techniques for Fraudulent Credit Card Transactions**\n",
        "\n",
        "# **1. Environment Setup**\n",
        "\n",
        "**1.1 Tools and Libraries Installation**\n",
        "\"\"\"\n",
        "\n",
        "!pip install lime\n",
        "!pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "\"\"\"# 2. **Importing Libraries**\n",
        "\n",
        "**2.1 Essential Libraries for Data Analysis**\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "\"\"\"**2.2 Libraries for Machine Learning and Visualization**\"\"\"\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "\"\"\"#3. **Loading Data**\n",
        "\n",
        "**3.1 Loading the Kaggle Credit Card Fraud Dataset**\n",
        "\"\"\"\n",
        "\n",
        "cfd = pd.read_csv('creditcard.csv')\n",
        "\n",
        "\"\"\"#4. **Exploration of Data**\n",
        "\n",
        "### **Glimpse of the Dataset**\n",
        "\n",
        "**4.1 Displaying the First Few Rows**\n",
        "\"\"\"\n",
        "\n",
        "cfd\n",
        "\n",
        "\"\"\"**4.1.2 Dataset Information (Shape, Columns, Null Values, Data Types)**\"\"\"\n",
        "\n",
        "print(f\"Shape:  {cfd.shape}\\n\")\n",
        "print(f\"Columns:  {cfd.columns}\\n\")\n",
        "print(f\"Null Values:  \\n{cfd.isnull().sum()}\\n\")\n",
        "print(f\"Data Types: \\n{cfd.dtypes}\")\n",
        "\n",
        "\"\"\"**4.2 Summary Statistics for Numerical Features**\"\"\"\n",
        "\n",
        "print(cfd.describe(include='all'))\n",
        "\n",
        "\"\"\"**4.2.1 Class Distribution (Fraud vs. Non-Fraud)**\"\"\"\n",
        "\n",
        "class_distribution = cfd['Class'].value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Class', data=cfd, hue='Class', palette=['blue', 'red'], legend=False)\n",
        "plt.title('Class Distribution (Fraud vs. Non-Fraud)')\n",
        "plt.xlabel('Class (0: Non-Fraud, 1: Fraud)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"### **Distribution of Independent Variable**\n",
        "\n",
        "**4.3 Distribution of Amount**\n",
        "\"\"\"\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.violinplot(x=cfd[\"Amount\"], color=\"blue\")  # Use a valid color name\n",
        "plt.title(\"Distribution of Transaction Amounts\")\n",
        "plt.xlabel(\"Amount\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**4.4 Distribution of Time**\"\"\"\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(cfd['Time'], kde=False, color=\"blue\")\n",
        "plt.title('Distribution of Transaction Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**4.5 Histograms for Key Features (V1-V28, Amount, Time)**\"\"\"\n",
        "\n",
        "cfd.hist(bins=30, figsize=(20, 15), edgecolor='black')\n",
        "plt.suptitle(\"Histograms of All Numerical Features\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"# **5. Pre-processing of Data**\n",
        "\n",
        "**5.1 Checking of Null Values**\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nChecking for Null Values:\")\n",
        "cfd.isnull().sum()\n",
        "\n",
        "\"\"\"**5.2 Checking of Outliers**\"\"\"\n",
        "\n",
        "Q1 = cfd['Amount'].quantile(0.25)\n",
        "Q3 = cfd['Amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers = cfd[(cfd['Amount'] < (Q1 - 1.5 * IQR)) | (cfd['Amount'] > (Q3 + 1.5 * IQR))]\n",
        "print(f\"Number of outliers in 'Amount': {len(outliers)}\")\n",
        "\n",
        "\"\"\"**5.3 Checking of Duplicate Transactions**\"\"\"\n",
        "\n",
        "cfd.duplicated()\n",
        "\n",
        "duplicate_counts = cfd.duplicated().value_counts()\n",
        "print(duplicate_counts)\n",
        "\n",
        "duplicate_counts = cfd.duplicated().value_counts()\n",
        "print(\"Duplicate Counts before removal:\\n\", duplicate_counts)\n",
        "\n",
        "cfd = cfd.drop_duplicates(keep='first')\n",
        "\n",
        "duplicate_counts = cfd.duplicated().value_counts()\n",
        "print(\"\\nDuplicate Counts after removal:\\n\", duplicate_counts)\n",
        "\n",
        "\"\"\"**5.4 Feature Selection/Reduction**\n",
        "\n",
        "**5.4.1 Correlation Matrix for Numerical Features**\n",
        "\"\"\"\n",
        "\n",
        "correlation_matrix = cfd.corr()\n",
        "\n",
        "\"\"\"**5.4.2 Heatmap Visualization**\"\"\"\n",
        "\n",
        "plt.figure(figsize=(28, 28))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**5.4.3 Dropping Irrelevant Features**\"\"\"\n",
        "\n",
        "threshold = 0.8\n",
        "high_corr_features = set()\n",
        "correlation_matrix = cfd.corr()\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "print(\"Highly correlated features to drop:\", high_corr_features)\n",
        "\n",
        "cfd.drop(columns=high_corr_features, inplace=True)\n",
        "print(f\"Updated dataset shape: {cfd.shape}\")\n",
        "\n",
        "\"\"\"### **Application of Standard Scaler**\n",
        "\n",
        "**5.5 Feature Scaling**\n",
        "\n",
        "**5.5.1 Standardization (Z-Score Scaling)**\n",
        "\"\"\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "cfd[['Amount']] = scaler.fit_transform(cfd[['Amount']])\n",
        "\n",
        "cfd['Amount'].describe()\n",
        "\n",
        "\"\"\"**5.5.2 Normalization (Min-Max Scaling)**\"\"\"\n",
        "\n",
        "time = cfd['Time']\n",
        "cfd['Time'] = (time - time.min()) / (time.max() - time.min())\n",
        "\n",
        "cfd\n",
        "\n",
        "cfd = cfd.sample(frac=1, random_state=1)\n",
        "cfd\n",
        "\n",
        "\"\"\"**5.6 Train, Test, and Validation**\"\"\"\n",
        "\n",
        "x = cfd.drop(columns=['Class'])\n",
        "y = cfd['Class']\n",
        "\n",
        "\"\"\"**5.7 Splitting the Dataset into Training and Testing Sets**\"\"\"\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\"\"\"# **6. Machine Learning**\n",
        "\n",
        "### **Isolation Forest**\n",
        "\"\"\"\n",
        "\n",
        "fraud_ratio = y_train.mean()\n",
        "if_model = IsolationForest(contamination=0.02, random_state=101)\n",
        "if_model.fit(x_train)\n",
        "\n",
        "fraud_test = x_test[y_test == 1]\n",
        "non_fraud_test = x_test[y_test == 0].sample(len(fraud_test), random_state=42)\n",
        "x_test_balanced = pd.concat([fraud_test, non_fraud_test])\n",
        "y_test_balanced = np.concatenate([np.ones(len(fraud_test)), np.zeros(len(non_fraud_test))])\n",
        "\n",
        "if_y_pred = (if_model.predict(x_test_balanced) == -1).astype(int)\n",
        "print(classification_report(y_test_balanced, if_y_pred))\n",
        "if_roc_auc = roc_auc_score(y_test_balanced, if_y_pred)\n",
        "print(\"ROC AUC Score:\", if_roc_auc)\n",
        "if_auprc = average_precision_score(y_test_balanced, if_y_pred)\n",
        "print(f\"AUPRC for Isolation Forest:\", if_auprc)\n",
        "\n",
        "report_dict = classification_report(y_test_balanced, if_y_pred, output_dict=True)\n",
        "\n",
        "if_accuracy = report_dict['accuracy']\n",
        "if_precision = report_dict['weighted avg']['precision']\n",
        "if_recall = report_dict['weighted avg']['recall']\n",
        "if_f1_score = report_dict['weighted avg']['f1-score']\n",
        "\n",
        "\"\"\"### **Autoencoders**\"\"\"\n",
        "\n",
        "y_train_fraud = y_train[y_train == 1].sample(frac=0.1, random_state=42)  # Increase to 10%\n",
        "x_train_fraud = x_train.loc[y_train_fraud.index]\n",
        "x_train_auto = pd.concat([x_train[y_train == 0], x_train_fraud])\n",
        "\n",
        "input_dim = x_train_auto.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "encoded = Dense(16, activation='relu')(encoded)\n",
        "encoded = Dense(8, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(16, activation='relu')(encoded)\n",
        "decoded = Dense(32, activation='relu')(decoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "autoencoder.fit(x_train_auto, x_train_auto, epochs=50, batch_size=256, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "reconstructed = autoencoder.predict(x_test)\n",
        "mse = np.mean(np.power(x_test - reconstructed, 2), axis=1)\n",
        "threshold = np.percentile(mse, 80)\n",
        "\n",
        "y_test_pred = (mse > threshold).astype(int)\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "ae_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
        "print(\"ROC AUC Score:\", ae_roc_auc)\n",
        "ae_auprc = average_precision_score(y_test, y_test_pred)\n",
        "print(f\"AUPRC for Autoencoder:\", ae_roc_auc)\n",
        "\n",
        "report_dict2 = classification_report(y_test, y_test_pred, output_dict=True)\n",
        "\n",
        "ae_accuracy = report_dict2['accuracy']\n",
        "ae_precision = report_dict2['weighted avg']['precision']\n",
        "ae_recall = report_dict2['weighted avg']['recall']\n",
        "ae_f1_score = report_dict2['weighted avg']['f1-score']\n",
        "\n",
        "\"\"\"### **Local Outlier Factor**\"\"\"\n",
        "\n",
        "x_train_normal = x_train[y_train == 0]\n",
        "\n",
        "lof_model = LocalOutlierFactor(n_neighbors=50, contamination=0.01, novelty=True)\n",
        "lof_model.fit(x_train_normal)\n",
        "\n",
        "lof_scores = lof_model.decision_function(x_test)\n",
        "lof_threshold = np.percentile(lof_scores, 2)\n",
        "y_test_pred_lof = (lof_scores < lof_threshold).astype(int)\n",
        "\n",
        "print(classification_report(y_test, y_test_pred_lof))\n",
        "lof_roc_auc = roc_auc_score(y_test, y_test_pred_lof)\n",
        "print(\"ROC AUC Score:\", lof_roc_auc)\n",
        "lof_auprc = average_precision_score(y_test, y_test_pred_lof)\n",
        "print(\"AUPRC for LOF:\", lof_auprc)\n",
        "\n",
        "report_dict3 = classification_report(y_test, y_test_pred_lof, output_dict=True)\n",
        "\n",
        "lof_accuracy = report_dict3['accuracy']\n",
        "lof_precision = report_dict3['weighted avg']['precision']\n",
        "lof_recall = report_dict3['weighted avg']['recall']\n",
        "lof_f1_score = report_dict3['weighted avg']['f1-score']\n",
        "\n",
        "\"\"\"# **7. Evaluation of Model Performance**\n",
        "\n",
        "**7.1 Creation of Metrics-Data**\n",
        "\"\"\"\n",
        "\n",
        "metrics_data = {\n",
        "    'Model': ['Isolation Forest', 'Autoencoders', 'Local Outlier Factor'],\n",
        "    'Accuracy': [if_accuracy, ae_accuracy, lof_accuracy],\n",
        "    'Precision': [if_precision, ae_precision, lof_precision],\n",
        "    'Recall': [if_recall, ae_recall, lof_recall],\n",
        "    'F1-score': [if_f1_score, ae_f1_score, lof_f1_score],\n",
        "    'AUC-ROC': [if_roc_auc, ae_roc_auc, lof_roc_auc],\n",
        "    'AUPRC': [if_auprc, ae_auprc, lof_auprc]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "metrics_df.head()\n",
        "\n",
        "# Ensure x_test is properly scaled\n",
        "x_test[['Amount']] = scaler.transform(x_test[['Amount']])\n",
        "x_test['Time'] = (x_test['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "\n",
        "# Isolation Forest Predictions\n",
        "if_y_pred = (if_model.predict(x_test) == -1).astype(int)\n",
        "\n",
        "# Autoencoder Predictions\n",
        "mse = np.mean(np.power(x_test - autoencoder.predict(x_test), 2), axis=1)\n",
        "best_threshold = np.percentile(mse, 90)  # Set fraud threshold\n",
        "ae_y_pred = (mse > best_threshold).astype(int)\n",
        "\n",
        "# Local Outlier Factor Predictions\n",
        "# Use lof_model instead of lof\n",
        "lof_scores = lof_model.decision_function(x_test)\n",
        "lof_threshold = np.percentile(lof_scores, 5)  # Set fraud threshold\n",
        "lof_y_pred = (lof_scores < lof_threshold).astype(int)\n",
        "\n",
        "# Ensemble Voting (Weighted)\n",
        "weights = [0.4, 0.4, 0.2]  # Adjust based on model performance\n",
        "ensemble_preds = np.average([if_y_pred, ae_y_pred, lof_y_pred], axis=0, weights=weights)\n",
        "final_preds = (ensemble_preds > 0.4).astype(int)  # Lower threshold to improve fraud detection\n",
        "\n",
        "# Evaluation\n",
        "print(\"Evaluation for Ensemble Model (Voting)\")\n",
        "print(classification_report(y_test, final_preds))\n",
        "print(f\"AUC-ROC Score: {roc_auc_score(y_test, final_preds):.4f}\")\n",
        "print(f\"AUPRC Score: {average_precision_score(y_test, final_preds):.4f}\")\n",
        "\n",
        "\"\"\"**7.2 Selection of Best Performing Model**\"\"\"\n",
        "\n",
        "metrics_data = {\n",
        "    'Model': ['Isolation Forest', 'Autoencoders', 'Local Outlier Factor'],\n",
        "    'Accuracy': [0.83, 0.80, 0.97],\n",
        "    'Precision': [0.84, 0.99, 0.99],\n",
        "    'Recall': [0.83, 0.80, 0.97],\n",
        "    'F1-score': [0.82, 0.88, 0.98],\n",
        "    'AUC-ROC': [0.83, 0.84, 0.71],\n",
        "    'AUPRC': [0.80, 0.006, 0.0018]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Model', y='AUPRC', data=metrics_df, palette='viridis')  # Change y to 'AUPRC'\n",
        "plt.title('Model Comparison - AUPRC')  # Update title\n",
        "plt.ylabel('AUPRC')  # Update y-axis label\n",
        "plt.xlabel('Model')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC', 'AUPRC']:  # Include 'AUPRC'\n",
        "    sns.lineplot(x='Model', y=metric, data=metrics_df, label=metric, marker='o')\n",
        "\n",
        "plt.title('Model Comparison - All Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Model')\n",
        "plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**7.3 LIME Analysis**\"\"\"\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    x_train.values, feature_names=x_train.columns.tolist(),\n",
        "    class_names=['Non-Fraud', 'Fraud'], verbose=True, mode='classification'\n",
        ")\n",
        "\n",
        "\"\"\"**7.3.1 LIME Analysis for Isolation Forest**\"\"\"\n",
        "\n",
        "def iforest_predict_proba(X):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X, columns=x_train.columns)\n",
        "\n",
        "    scores = if_model.decision_function(X)\n",
        "    probs = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "    return np.vstack([1 - probs, probs]).T\n",
        "\n",
        "idx = np.random.randint(0, x_test.shape[0])\n",
        "exp = explainer.explain_instance(x_test.iloc[idx].values, iforest_predict_proba)\n",
        "\n",
        "exp.show_in_notebook()\n",
        "\n",
        "feature_importances = []\n",
        "for _ in range(5):\n",
        "    exp = explainer.explain_instance(x_test.iloc[0].values, iforest_predict_proba)\n",
        "    feature_importances.extend(exp.as_list())\n",
        "\n",
        "feature_importance_df = pd.DataFrame(feature_importances, columns=['feature', 'importance'])\n",
        "avg_feature_importance_df = feature_importance_df.groupby('feature')['importance'].mean().reset_index()\n",
        "display(avg_feature_importance_df.style.hide(axis='index'))\n",
        "\n",
        "\"\"\"**7.3.2 LIME Analysis for Autoencoders**\"\"\"\n",
        "\n",
        "def autoencoder_predict_proba(X):\n",
        "    reconstructed = autoencoder.predict(X)\n",
        "    mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "    min_mse, max_mse = mse.min(), mse.max()\n",
        "    probs = (mse - min_mse) / (max_mse - min_mse)\n",
        "    return np.vstack([1 - probs, probs]).T\n",
        "\n",
        "idx = np.random.randint(0, x_test.shape[0])\n",
        "exp2 = explainer.explain_instance(x_test.iloc[idx].values, autoencoder_predict_proba)\n",
        "\n",
        "exp2.show_in_notebook()\n",
        "\n",
        "feature_importances = []\n",
        "for _ in range(5):\n",
        "    exp2 = explainer.explain_instance(x_test.iloc[0].values, autoencoder_predict_proba)\n",
        "    feature_importances.extend(exp.as_list())\n",
        "\n",
        "feature_importance_df = pd.DataFrame(feature_importances, columns=['feature', 'importance'])\n",
        "avg_feature_importance_df = feature_importance_df.groupby('feature')['importance'].mean().reset_index()\n",
        "display(avg_feature_importance_df.style.hide(axis='index'))\n",
        "\n",
        "\"\"\"**7.3.3 LIME Analysis for Local Outlier Factor**\"\"\"\n",
        "\n",
        "def lof_predict_proba(X):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X, columns=x_train.columns)\n",
        "    lof_scores = lof_model._predict(X)\n",
        "    probs = (lof_scores + 1) / 2\n",
        "    return np.vstack([1 - probs, probs]).T\n",
        "\n",
        "idx = np.random.randint(0, x_test.shape[0])\n",
        "exp3 = explainer.explain_instance(x_test.iloc[idx].values, lof_predict_proba)\n",
        "\n",
        "exp3.show_in_notebook()\n",
        "\n",
        "feature_importances = []\n",
        "for _ in range(5):\n",
        "    exp3 = explainer.explain_instance(x_test.iloc[0].values, lof_predict_proba)\n",
        "    feature_importances.extend(exp.as_list())\n",
        "\n",
        "feature_importance_df = pd.DataFrame(feature_importances, columns=['feature', 'importance'])\n",
        "avg_feature_importance_df = feature_importance_df.groupby('feature')['importance'].mean().reset_index()\n",
        "display(avg_feature_importance_df.style.hide(axis='index'))\n",
        "\n",
        "\"\"\"**7.4 Confusion Matrix for Each Model**\"\"\"\n",
        "\n",
        "if_cm = confusion_matrix(y_test_balanced, if_y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(if_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix - Isolation Forest')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "ae_cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(ae_cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix - Autoencoders')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "lof_cm = confusion_matrix(y_test, y_test_pred_lof)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(lof_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'], cbar=False)\n",
        "plt.title('Confusion Matrix - Local Outlier Factor')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**7.5 Cohen's Kappa**\"\"\"\n",
        "\n",
        "if_kappa = cohen_kappa_score(y_test_balanced, if_y_pred)\n",
        "print(f\"Cohen's Kappa for Isolation Forest: {if_kappa}\")\n",
        "\n",
        "ae_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
        "print(f\"Cohen's Kappa for Autoencoders: {ae_kappa}\")\n",
        "\n",
        "lof_kappa = cohen_kappa_score(y_test, y_test_pred_lof)\n",
        "print(f\"Cohen's Kappa for Local Outlier Factor: {lof_kappa}\")\n",
        "\n",
        "metrics_data['Cohen\\'s Kappa'] = [if_kappa, ae_kappa, lof_kappa]\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "metrics_df\n",
        "\n",
        "\"\"\"# **8. Detection of Fraud**\n",
        "\n",
        "**8.1 Defining Input Parameters (Time, Amount, V1-V28)**\n",
        "\n",
        "Enter transaction time: 100000\n",
        "\n",
        "Enter transaction amount: 5000.00\n",
        "\n",
        "Enter value for V1: -5.64\n",
        "\n",
        "Enter value for V2: -7.27\n",
        "\n",
        "Enter value for V3: -4.83\n",
        "\n",
        "Enter value for V4: -5.68\n",
        "\n",
        "Enter value for V5: -1.14\n",
        "\n",
        "Enter value for V6: -2.62\n",
        "\n",
        "Enter value for V7: -4.36\n",
        "\n",
        "Enter value for V8: -7.32\n",
        "\n",
        "Enter value for V9: -1.34\n",
        "\n",
        "Enter value for V10: -0.02\n",
        "\n",
        "Enter value for V11: 0.28\n",
        "\n",
        "Enter value for V12: -0.23\n",
        "\n",
        "Enter value for V13: -0.64\n",
        "\n",
        "Enter value for V14: 0.10\n",
        "\n",
        "Enter value for V15: 0.17\n",
        "\n",
        "Enter value for V16: 0.13\n",
        "\n",
        "Enter value for V17: -0.01\n",
        "\n",
        "Enter value for V18: 0.01\n",
        "\n",
        "Enter value for V19: -0.11\n",
        "\n",
        "Enter value for V20: 0.07\n",
        "\n",
        "Enter value for V21: 0.13\n",
        "\n",
        "Enter value for V22: -0.19\n",
        "\n",
        "Enter value for V23: 0.13\n",
        "\n",
        "Enter value for V24: -0.02\n",
        "\n",
        "Enter value for V25: 0.13\n",
        "\n",
        "Enter value for V26: -0.19\n",
        "\n",
        "Enter value for V27: 0.13\n",
        "\n",
        "Enter value for V28: -0.02\n",
        "\n",
        "Expected Output: Fraudulent\n",
        "\n",
        "**8.2 Preprocessing Input Data**\n",
        "\n",
        "**8.3 Function for Fraud Prediction**\n",
        "\"\"\"\n",
        "\n",
        "def predict_fraud(input_data):\n",
        "    decision_score = if_model.decision_function(input_data)\n",
        "    fraud_prediction = (decision_score < 0).astype(int)[0]  # Adjusted threshold\n",
        "    return \"Fraudulent\" if fraud_prediction == 1 else \"Non-Fraudulent\"\n",
        "\n",
        "\"\"\"**8.4 Prediction using Isolation Forest**\"\"\"\n",
        "\n",
        "def fraudulent_data():\n",
        "    time = 100000\n",
        "    amount = 5000.00\n",
        "    v_values = [-5.64, -7.27, -4.83, -5.68, -1.14, -2.62, -4.36, -7.32, -1.34, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data = fraudulent_data()\n",
        "input_data[['Amount']] = scaler.transform(input_data[['Amount']])  # Apply same scaling\n",
        "input_data['Time'] = (input_data['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "input_data = input_data[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", predict_fraud(input_data))\n",
        "\n",
        "def non_fraudulent_data():\n",
        "    time = 50000\n",
        "    amount = 50.00\n",
        "    v_values = [-1.36, -0.07, 2.54, 1.38, -0.34, 0.46, 0.24, 0.10, 0.36, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data2 = non_fraudulent_data()\n",
        "input_data2[['Amount']] = scaler.transform(input_data2[['Amount']])  # Apply same scaling\n",
        "input_data2['Time'] = (input_data2['Time'] - time.min()) / (time.max() - time.min())\n",
        "input_data2 = input_data2[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", predict_fraud(input_data2))\n",
        "\n",
        "\"\"\"**8.5 Prediction using Autoencoders**\"\"\"\n",
        "\n",
        "def ae_predict_fraud(input_data):\n",
        "    reconstructed = autoencoder.predict(input_data)\n",
        "    mse = np.mean(np.power(input_data - reconstructed, 2), axis=1)\n",
        "    fraud_prediction = (mse > threshold).astype(int)[0]\n",
        "    return \"Fraudulent\" if fraud_prediction == 1 else \"Non-Fraudulent\"\n",
        "\n",
        "def fraudulent_data():\n",
        "    time = 100000\n",
        "    amount = 5000.00\n",
        "    v_values = [-5.64, -7.27, -4.83, -5.68, -1.14, -2.62, -4.36, -7.32, -1.34, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data3 = fraudulent_data()\n",
        "input_data3[['Amount']] = scaler.transform(input_data3[['Amount']])  # Apply same scaling\n",
        "input_data3['Time'] = (input_data3['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "input_data3 = input_data3[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", ae_predict_fraud(input_data3))\n",
        "\n",
        "def non_fraudulent_data():\n",
        "    time = 50000\n",
        "    amount = 50.00\n",
        "    v_values = [-1.36, -0.07, 2.54, 1.38, -0.34, 0.46, 0.24, 0.10, 0.36, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data4 = non_fraudulent_data()\n",
        "input_data4[['Amount']] = scaler.transform(input_data4[['Amount']])  # Apply same scaling\n",
        "input_data4['Time'] = (input_data4['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "input_data4 = input_data4[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", ae_predict_fraud(input_data4))\n",
        "\n",
        "\"\"\"**8.6 Prediction using Local Outlier Factor**\"\"\"\n",
        "\n",
        "def lof_predict_fraud(input_data):\n",
        "    input_data[['Amount']] = scaler.transform(input_data[['Amount']])  # Apply same scaling to new transactions\n",
        "    input_data['Time'] = (input_data['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min() + 1e-6)  # Ensure Time normalization\n",
        "    lof_score = lof_model.decision_function(input_data)\n",
        "    fraud_prediction = (lof_score < lof_threshold).astype(int)[0]\n",
        "    return \"Fraudulent\" if fraud_prediction == 1 else \"Non-Fraudulent\"\n",
        "\n",
        "def fraudulent_data():\n",
        "    time = 100000\n",
        "    amount = 5000.00\n",
        "    v_values = [-5.64, -7.27, -4.83, -5.68, -1.14, -2.62, -4.36, -7.32, -1.34, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data5 = fraudulent_data()\n",
        "input_data5[['Amount']] = scaler.transform(input_data5[['Amount']])  # Apply same scaling\n",
        "input_data5['Time'] = (input_data5['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "input_data5 = input_data5[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", lof_predict_fraud(input_data5))\n",
        "\n",
        "def non_fraudulent_data():\n",
        "    time = 50000\n",
        "    amount = 50.00\n",
        "    v_values = [-1.36, -0.07, 2.54, 1.38, -0.34, 0.46, 0.24, 0.10, 0.36, -0.02, 0.28, -0.23, -0.64, 0.10, 0.17, 0.13, -0.01, 0.01, -0.11, 0.07, 0.13, -0.19, 0.13, -0.02, 0.13, -0.19, 0.13, -0.02]\n",
        "    if len(v_values) == 28:\n",
        "        return pd.DataFrame([[time, amount] + v_values], columns=['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)])\n",
        "    else:\n",
        "        print(\"Error: v_values does not contain 28 elements\")\n",
        "\n",
        "input_data6 = non_fraudulent_data()\n",
        "input_data6[['Amount']] = scaler.transform(input_data6[['Amount']])  # Apply same scaling\n",
        "input_data6['Time'] = (input_data6['Time'] - cfd['Time'].min()) / (cfd['Time'].max() - cfd['Time'].min())\n",
        "input_data6 = input_data6[x_train.columns]  # Ensure correct feature order\n",
        "\n",
        "print(\"Transaction Prediction:\", lof_predict_fraud(input_data6))"
      ],
      "metadata": {
        "id": "CvrGvRsvK7GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}